{"meta":{"title":"Xue's Homepage","subtitle":null,"description":"A Computer Science student in Dalian University of Technology","author":"Shiqing Xue","url":"http://yoursite.com"},"pages":[{"title":"分类","date":"2018-10-08T14:29:05.146Z","updated":"2018-05-13T15:33:24.412Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"书单","date":"2018-10-08T14:29:05.145Z","updated":"2018-05-13T15:33:24.412Z","comments":false,"path":"books/index.html","permalink":"http://yoursite.com/books/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2018-10-08T14:29:05.149Z","updated":"2018-05-13T15:33:24.412Z","comments":false,"path":"repository/index.html","permalink":"http://yoursite.com/repository/index.html","excerpt":"","text":""},{"title":"404 Not Found：该页无法显示","date":"2018-10-08T14:29:05.144Z","updated":"2018-05-13T15:33:24.412Z","comments":false,"path":"/404.html","permalink":"http://yoursite.com//404.html","excerpt":"","text":""},{"title":"友情链接","date":"2018-10-08T14:29:05.148Z","updated":"2018-05-13T15:33:24.412Z","comments":true,"path":"links/index.html","permalink":"http://yoursite.com/links/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-10-08T14:29:05.150Z","updated":"2018-05-13T15:33:24.412Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"kubernetes的调度方法_一","slug":"4_kubernetes的调度方法_一","date":"2018-10-19T14:04:06.000Z","updated":"2018-10-19T15:07:31.212Z","comments":true,"path":"2018/10/19/4_kubernetes的调度方法_一/","link":"","permalink":"http://yoursite.com/2018/10/19/4_kubernetes的调度方法_一/","excerpt":"","text":"kubernetes的调度方法参考文献《kubernetes权威指南——从Docker到kubernetes实践全接触》 http://dockone.io/article/2885 作者：薛世卿注：本文包含了对kubernetes调度器的调度策略、优选部分的介绍。 背景介绍Kubernetes的架构设计基本上是参照了Google Borg。Google的Borg系统群集管理器负责管理几十万个以上的jobs，来自几千个不同的应用，跨多个集群，每个集群有上万个机器。它通过管理控制、高效的任务包装、超售、和进程级别性能隔离实现了高利用率。它支持高可用性应用程序与运行时功能，最大限度地减少故障恢复时间，减少相关故障概率的调度策略。 基于资源分配的任务调度是Kubernetes的核心组件。Kubernetes的调度策略源自Borg, 但是为了更好的适应新一代的容器应用，以及各种规模的部署，Kubernetes的调度策略相应做的更加灵活，也更加容易理解和使用。 Kubernetes基本架构如下： 其中， Controller Manager主要用于管理计算节点（Node Controller）以及Pod副本（Replication Controller）等，Scheduler根据特定的算法和策略调度Pod到具体的计算节点，Kubelet通过apiserver或者监控本地的配置文件（eg. Kubeadm创建的Kuernetes集群），通过docker daemon创建Pod的container。 调度策略Kubernetes的调度器以插件化形式实现的，方便用户定制和二次开发。用户可以自定义调度器并以插件形式与Kubernetes集成，或集成其他调度器，便于调度不同类型的任务。 Kubernetes调度器的源码位于kubernetes/plugin/中，大体的代码目录结构如下所示： kubernetes/plugin/pkg/ `-- scheduler //调度相关的具体实现 |-- algorithm | |-- predicates //节点筛选策略 | `-- priorities //节点打分策略 | `-- util |-- algorithmprovider | `-- defaults //定义默认的调度器 调度分为几个部分：首先是预选过程，过滤掉不满足条件的节点，这个过程称为Predicates；然后是优选过程，对通过的节点按照优先级排序，称之为Priorities；最后从中选择优先级最高的节点。如果中间任何一步骤有错误，就直接返回错误。 进一步说： Predicates阶段回答“能不能”的问题：首先遍历全部节点，过滤掉不满足条件的节点，这一阶段输出的所有满足要求的Node将被记录并作为第二阶段的输入。 Priorities阶段是回答“哪个更适合的问题”：即再次对节点进行筛选，筛选出最适合运行Pod的节点。 如果在预选（Predicates）过程中，如果所有的节点都不满足条件，Pod 会一直处在Pending 状态，直到有节点满足条件，这期间调度器会不断的重试。经过节点过滤后，如多个节点满足条件，会按照节点优先级（priorities）大小对节点排序，最后选择优先级最高的节点部署Pod。 具体的调度过程，一般如下： 首先，客户端通过API Server的REST API/kubectl/helm创建pod/service/deployment/job等，支持类型主要为JSON/YAML/helm tgz。 接下来，API Server收到用户请求，存储到相关数据到etcd。 调度器通过API Server查看未调度（bind）的Pod列表，循环遍历地为每个Pod分配节点，尝试为Pod分配节点。调度过程分为2个阶段： 第一阶段：预选过程，过滤节点，调度器用一组规则过滤掉不符合要求的主机。比如Pod指定了所需要的资源量，那么可用资源比Pod需要的资源量少的主机会被过滤掉。 第二阶段：优选过程，节点优先级打分，对第一步筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略，比如把容一个Replication Controller的副本分布到不同的主机上，使用最低负载的主机等。 选择主机：选择打分最高的节点，进行binding操作，结果存储到etcd中。 所选节点对于的kubelet根据调度结果执行Pod创建操作。 Kubernetes调度器使用Predicates和Priorites来决定一个Pod应该运行在哪一个节点上。Predicates是强制性规则，用来形容主机匹配Pod所需要的资源，如果没有任何主机满足该Predicates，则该Pod会被挂起，直到有节点能够满足调度条件。 优选经过预选策略（Predicates）对节点过滤后，获取节点列表，再对符合需求的节点列表进行打分，最终选择Pod调度到一个分值最高的节点。Kubernetes用一组优先级函数处理每一个通过预选的节点（kubernetes/plugin/pkg/scheduler/algorithm/priorities中实现）。每一个优先级函数会返回一个0-10的分数，分数越高表示节点越优， 同时每一个函数也会对应一个表示权重的值。这好比有许多评委同时对节点进行打分，不同的评委有着不同的权重，最终主机的得分用以下公式计算得出： finalScoreNode = (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + … + (weightn * priorityFuncn) 对应代码位于kubernetes/plugin/pkg/scheduler/core/generic_scheduler.go[650],combinedScores即为节点总分。 if len(extenders) != 0 &amp;&amp; nodes != nil { combinedScores := make(map[string]int, len(nodeNameToInfo)) for _, extender := range extenders { if !extender.IsInterested(pod) { continue } wg.Add(1) go func(ext algorithm.SchedulerExtender) { defer wg.Done() prioritizedList, weight, err := ext.Prioritize(pod, nodes) if err != nil { // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities return } mu.Lock() for i := range *prioritizedList { host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score combinedScores[host] += score * weight } mu.Unlock() }(extender) } 目前支持优选的优先级函数包括以下几种：LeastRequestedPriority：节点的优先级就由节点空闲资源与节点总容量的比值，即由（总容量-节点上Pod的容量总和-新Pod的容量）/总容量）来决定。CPU和内存具有相同权重，资源空闲比越高的节点得分越高。需要注意的是，这个优先级函数起到了按照资源消耗来跨节点分配Pod的作用。详细的计算规则如下： cpu((capacity – sum(requested)) * 10 / capacity) + memory((capacity – sum(requested)) * 10 / capacity) / 2 注：10 表示非常合适，0 表示完全不合适。 LeastRequestedPriority举例说明：例如CPU的可用资源为100，运行容器申请的资源为15，则cpu分值为8.5分，内存可用资源为100，运行容器申请资源为20，则内存分支为8分。则此评价规则在此节点的分数为(8.5 +8) / 2 = 8.25分。 BalancedResourceAllocation：CPU和内存使用率越接近的节点权重越高，该策略不能单独使用，必须和LeastRequestedPriority组合使用，尽量选择在部署Pod后各项资源更均衡的机器。如果请求的资源（CPU或者内存）大于节点的capacity，那么该节点永远不会被调度到。 BalancedResourceAllocation举例说明：该调度策略是出于平衡度的考虑，避免出现CPU，内存消耗不均匀的事情。例如某节点的CPU剩余资源还比较充裕，假如为100，申请10，则cpuFraction为0.1，而内存剩余资源不多，假如为20，申请10，则memoryFraction为0.5，这样由于CPU和内存使用不均衡，此节点的得分为10-abs ( 0.1 - 0.5 ) * 10 = 6 分。假如CPU和内存资源比较均衡，例如两者都为0.5，那么代入公式，则得分为10分。 InterPodAffinityPriority：通过迭代 weightedPodAffinityTerm 的元素计算和，并且如果对该节点满足相应的PodAffinityTerm，则将 “weight” 加到和中，具有最高和的节点是最优选的。 SelectorSpreadPriority：为了更好的容灾，对同属于一个service、replication controller或者replica的多个Pod副本，尽量调度到多个不同的节点上。如果指定了区域，调度器则会尽量把Pod分散在不同区域的不同节点上。当一个Pod的被调度时，会先查找Pod对于的service或者replication controller，然后查找service或replication controller中已存在的Pod，运行Pod越少的节点的得分越高。 SelectorSpreadPriority举例说明：这里主要针对多实例的情况下使用。例如，某一个服务，可能存在5个实例，例如当前节点已经分配了2个实例了，则本节点的得分为10（（5-2）/ 5）=6分，而没有分配实例的节点，则得分为10 （（5-0） / 5）=10分。没有分配实例的节点得分越高。 注：1.0版本被称之为ServiceSpreadingPriority，1.0之后版本变更为SelectorSpreadPriority，为了向前兼容ServiceSpreadingPriority名称仍然保留。 NodeAffinityPriority：Kubernetes调度中的亲和性机制。Node Selectors（调度时将pod限定在指定节点上），支持多种操作符（In, NotIn, Exists, DoesNotExist, Gt, Lt），而不限于对节点labels的精确匹配。另外，Kubernetes支持两种类型的选择器，一种是“hard（requiredDuringSchedulingIgnoredDuringExecution）”选择器，它保证所选的主机必须满足所有Pod对主机的规则要求。这种选择器更像是之前的nodeselector，在nodeselector的基础上增加了更合适的表现语法。另一种是“soft（preferresDuringSchedulingIgnoredDuringExecution）”选择器，它作为对调度器的提示，调度器会尽量但不保证满足NodeSelector的所有要求。 NodePreferAvoidPodsPriority（权重1W）：如果 节点的 Anotation 没有设置 key-value:scheduler. alpha.kubernetes.io/ preferAvoidPods = “…”，则节点对该 policy 的得分就是10分，加上权重10000，那么该node对该policy的得分至少10W分。如果Node的Anotation设置了，scheduler.alpha.kubernetes.io/preferAvoidPods = “…” ，如果该 pod 对应的 Controller 是 ReplicationController 或 ReplicaSet，则该 node 对该 policy 的得分就是0分。 TaintTolerationPriority : 使用 Pod 中 tolerationList 与 节点 Taint 进行匹配，配对成功的项越多，则得分越低。 另外在优选的调度规则中，有几个未被默认使用的规则： ImageLocalityPriority：根据Node上是否存在一个pod的容器运行所需镜像大小对优先级打分，分值为0-10。遍历全部Node，如果某个Node上pod容器所需的镜像一个都不存在，分值为0；如果Node上存在Pod容器部分所需镜像，则根据这些镜像的大小来决定分值，镜像越大，分值就越高；如果Node上存在pod所需全部镜像，分值为10。 EqualPriority : EqualPriority 是一个优先级函数，它给予所有节点相等权重。 MostRequestedPriority : 在 ClusterAutoscalerProvider 中，替换 LeastRequestedPriority，给使用多资源的节点，更高的优先级。计算公式为：(cpu(10 sum(requested) / capacity) + memory(10 sum(requested) / capacity)) / 2 要想获得所有节点最终的权重分值，就要先计算每个优先级函数对应该节点的分值，然后计算总和。因此不管过程如何，如果有 N 个节点，M 个优先级函数，一定会计算 M*N 个中间值，构成一个二维表格： 最后，会把表格中按照节点把优先级函数的权重列表相加，得到最终节点的分值。上面代码就是这个过程，中间过程可以并发计算，以加快速度。 优选算法实例我们以taint_toleration.go[55]为例来看一看 // ComputeTaintTolerationPriorityMap prepares the priority list for all the nodes based on the number of intolerable taints on the node func ComputeTaintTolerationPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(&quot;node not found&quot;) } // To hold all the tolerations with Effect PreferNoSchedule var tolerationsPreferNoSchedule []v1.Toleration if priorityMeta, ok := meta.(*priorityMetadata); ok { tolerationsPreferNoSchedule = priorityMeta.podTolerations } else { tolerationsPreferNoSchedule = getAllTolerationPreferNoSchedule(pod.Spec.Tolerations) } return schedulerapi.HostPriority{ Host: node.Name, Score: countIntolerableTaintsPreferNoSchedule(node.Spec.Taints, tolerationsPreferNoSchedule), }, nil } // ComputeTaintTolerationPriorityReduce calculates the source of each node based on the number of intolerable taints on the node var ComputeTaintTolerationPriorityReduce = NormalizeReduce(schedulerapi.MaxPriority, true) 每一个优选算法都包含两个需要定义的方法，一个是mapFunction，另一个是reduceFunction，其中map是用来得到每个节点所获得的绝对分数，reduce通常对分数做一个归一化处理。map方法需要返回一个schedulerapi.HostPriority类型的结构体，包含了节点名称和分数。reduce方法返回一个标准化的值，本例中的reduce方法直接使用了系统自带的标准化方法NormalizeReduce，其作用是将绝对分数归到[0, maxPriority]的范围，如果reverse参数为true，分数再经过以下方式处理： if reverse { score = maxPriority - score } 默认优选策略default.go[217] func defaultPriorities() sets.String { return sets.NewString( // spreads pods by minimizing the number of pods (belonging to the same service or replication controller) on the same node. factory.RegisterPriorityConfigFactory( &quot;SelectorSpreadPriority&quot;, factory.PriorityConfigFactory{ MapReduceFunction: func(args factory.PluginFactoryArgs) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { return priorities.NewSelectorSpreadPriority(args.ServiceLister, args.ControllerLister, args.ReplicaSetLister, args.StatefulSetLister) }, Weight: 1, }, ), // pods should be placed in the same topological domain (e.g. same node, same rack, same zone, same power domain, etc.) // as some other pods, or, conversely, should not be placed in the same topological domain as some other pods. factory.RegisterPriorityConfigFactory( &quot;InterPodAffinityPriority&quot;, factory.PriorityConfigFactory{ Function: func(args factory.PluginFactoryArgs) algorithm.PriorityFunction { return priorities.NewInterPodAffinityPriority(args.NodeInfo, args.NodeLister, args.PodLister, args.HardPodAffinitySymmetricWeight) }, Weight: 1, }, ), // Prioritize nodes by least requested utilization. factory.RegisterPriorityFunction2(&quot;LeastRequestedPriority&quot;, priorities.LeastRequestedPriorityMap, nil, 1), // Prioritizes nodes to help achieve balanced resource usage factory.RegisterPriorityFunction2(&quot;BalancedResourceAllocation&quot;, priorities.BalancedResourceAllocationMap, nil, 1), // Set this weight large enough to override all other priority functions. // TODO: Figure out a better way to do this, maybe at same time as fixing #24720. factory.RegisterPriorityFunction2(&quot;NodePreferAvoidPodsPriority&quot;, priorities.CalculateNodePreferAvoidPodsPriorityMap, nil, 10000), // Prioritizes nodes that have labels matching NodeAffinity factory.RegisterPriorityFunction2(&quot;NodeAffinityPriority&quot;, priorities.CalculateNodeAffinityPriorityMap, priorities.CalculateNodeAffinityPriorityReduce, 1), // Prioritizes nodes that marked with taint which pod can tolerate. factory.RegisterPriorityFunction2(&quot;TaintTolerationPriority&quot;, priorities.ComputeTaintTolerationPriorityMap, priorities.ComputeTaintTolerationPriorityReduce, 1), ) } 这里注册的算法都是默认的优选策略，我们只需要调整权重就可以做到对优选策略的自由组合，如果不想启用某种策略，可以直接将其权重（Weight）设置为0. 事实上，还可以通过文件方式来组合优选策略： config.json &quot;kind&quot; : &quot;Policy&quot;, &quot;apiVersion&quot; : &quot;v1&quot;, &quot;predicates&quot; : [ {&quot;name&quot; : &quot;PodFitsHostPorts&quot;}, {&quot;name&quot; : &quot;PodFitsResources&quot;}, {&quot;name&quot; : &quot;NoDiskConflict&quot;}, {&quot;name&quot; : &quot;NoVolumeZoneConflict&quot;}, {&quot;name&quot; : &quot;MatchNodeSelector&quot;}, {&quot;name&quot; : &quot;HostName&quot;} ], &quot;priorities&quot; : [ {&quot;name&quot; : &quot;LeastRequestedPriority&quot;, &quot;weight&quot; : 1}, {&quot;name&quot; : &quot;BalancedResourceAllocation&quot;, &quot;weight&quot; : 1}, {&quot;name&quot; : &quot;ServiceSpreadingPriority&quot;, &quot;weight&quot; : 1}, {&quot;name&quot; : &quot;EqualPriority&quot;, &quot;weight&quot; : 1} ], &quot;hardPodAffinitySymmetricWeight&quot; : 10 此时调度器启动方式 kube-scheduler --policy-config-file config.json","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"使用kubeadm快速调试kubernetes源代码","slug":"3_使用kubeadm快速调试kubernetes源代码","date":"2018-10-08T15:33:15.885Z","updated":"2018-10-08T15:02:51.466Z","comments":true,"path":"2018/10/08/3_使用kubeadm快速调试kubernetes源代码/","link":"","permalink":"http://yoursite.com/2018/10/08/3_使用kubeadm快速调试kubernetes源代码/","excerpt":"","text":"使用kubeadm快速调试kubernetes源代码参考kubernetes官方文档https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ 作者：薛世卿如遇安装问题，可以咨询我。 重要阅读本文前，请确保集群按照我的上一篇文章《使用kubeadm搭建kubernetes集群》完成了集群的搭建。 步骤如下假设我们现在有三个节点，Node1、Node2、Node3。且Node1作为我们kubernetes的master节点。 我们修改了kubernetes的一部分源码，重新编译后生成镜像，并将名字替换为官方镜像的名称。 docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-scheduler-amd64 v1.10.4 c02501f0faad 11 days ago 50.3 MB 之后，我们想让新的kube-scheduler生效 注：如果安装了DashBoard，可以提前删除组件 kubectl apply -f dashboard-admin.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 其中，dashboard-admin.yaml文件内容在《使用kubeadm搭建kubernetes集群》中，只需使用当时安装的文件进行删除即可 集群卸载在三个节点上，运行 kubeadm reset 此时kubernetes集群已经卸载 集群重新安装在Node1(master)上，运行 kubeadm init --kubernetes-version=1.10.4 --pod-network-cidr=10.244.0.0/16 等待初始化完毕，记录下最后一句话 kubeadm join --token d405c1.18b51150e22ffe72 192.168.128.26:6443 --discovery-token-ca-cert-hash sha256:936229f8381de8df72e8b0de8a349a0099f0d0fc0407ca17a5bffe2e6 在Node2、Node3上执行上述命令，正常情况下，输出成功信息。 安装flannel此时节点状态应该仍为not ready，执行 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml 查看节点 kubectl get nodes 此时master节点不作为工作节点，如果你希望pods也能够调度到master节点上，执行以下命令 kubectl taint nodes --all node-role.kubernetes.io/master- 之后的配置请参考《使用kubeadm搭建kubernetes集群》，本文不再赘述。","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"OpenStack单节点安装","slug":"1_OpenStack单节点安装","date":"2018-10-08T14:29:05.116Z","updated":"2018-06-02T05:25:44.115Z","comments":true,"path":"2018/10/08/1_OpenStack单节点安装/","link":"","permalink":"http://yoursite.com/2018/10/08/1_OpenStack单节点安装/","excerpt":"","text":"OpenStack单节点单网卡安装参考陈沙克日志http://www.chenshake.com/ubuntu-12-04-openstack-essex-installation-single-node/ 作者：薛世卿如遇安装问题，可以咨询我。 注意，本文档不包含陈沙克的可选安装部分，这部分内容包括 ntp服务器 ISCSI（供Nova-volume使用） Nova-volume 一、准备系统系统信息我安装的节点为 IVIC 云主机 IP为 10.210.0.94 开始安装只需确保系统安装ssh server即可，然后更新源 apt-get update &amp;&amp; apt-get -y dist-upgrade 设置网络编辑/etc/network/interfaces，IP地址和DNS根据实际情况调整。 auto lo iface lo inet loopback auto eth0 iface eth0 inet static address 10.210.0.94 netmask 255.255.0.0 gateway 10.210.0.1 dns-nameserver 202.112.128.51 auto eth0:0 iface eth0:0 inet manual up ifconfig eth0:0 up Bridge使用linux的bridge和iptables来实现OpenStack的网络配置 apt-get -y install bridge-utils 环境变量你可以根据你的实际情况修改admin的密码和mysql的密码。下面文档和数据库相关的密码都是相同，你只需要修改novarc就可以。 运行完下面的命令，你再对novarc进行修改。 cat &gt;/root/novarc &lt;&lt;EOF export OS_TENANT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=password export MYSQL_PASS=password export SERVICE_PASSWORD=password export FIXED_RANGE=10.0.0.0/24 export FLOATING_RANGE=$(/sbin/ifconfig eth0 | awk &apos;/inet addr/ {print $2}&apos; | cut -f2 -d &quot;:&quot; | awk -F &quot;.&quot; &apos;{print $1&quot;.&quot;$2&quot;.&quot;$3}&apos;).224/27 export OS_AUTH_URL=&quot;http://localhost:5000/v2.0/&quot; export SERVICE_ENDPOINT=&quot;http://localhost:35357/v2.0&quot; export SERVICE_TOKEN=$(openssl rand -hex 10) export MASTER=&quot;$(/sbin/ifconfig eth0 | awk &apos;/inet addr/ {print $2}&apos; | cut -f2 -d &quot;:&quot;)&quot; EOF 我的novarc内容，与原博客不同的是，出于某种原因我注释掉了两行，具体原因我也不是很清楚了2333。 ubuntu@mu:~$ sudo cat /root/novarc export OS_TENANT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=password export MYSQL_PASS=password export SERVICE_PASSWORD=password export FIXED_RANGE=10.0.0.0/24 export FLOATING_RANGE=10.210.0.94/27 export OS_AUTH_URL=&quot;http://localhost:5000/v2.0/&quot; #export SERVICE_ENDPOINT=&quot;http://localhost:35357/v2.0&quot; #export SERVICE_TOKEN=a23debc939158820de0f export MASTER=&quot;10.210.0.94&quot; 确认没有问题或者进行修改，运行 source novarc echo &quot;source novarc&quot;&gt;&gt;.bashrc MYSQL在Openstack组件里，Nova，Keystone, Glance, 都需要用到数据库。所以我们需要创建相关的数据库和用户。 应用数据库数据库用户密码mysqlrootpasswordnovanovapasswordglanceglancepasswordkeystonekeystonepassword&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 安装mysql自动安装 cat &lt;&lt;MYSQL_PRESEED | debconf-set-selections mysql-server-5.5 mysql-server/root_password password $MYSQL_PASS mysql-server-5.5 mysql-server/root_password_again password $MYSQL_PASS mysql-server-5.5 mysql-server/start_on_boot boolean true MYSQL_PRESEED Openstack都是Python写的，所以你需要python-mysqldb，安装过程，就不会提示你输入root密码 apt-get install -y mysql-server python-mysqldb 配置编辑/etc/mysql/my.cnf, 允许网络访问mysql #bind-address = 127.0.0.1 bind-address = 0.0.0.0 或者直接运行下面命令 sed -i &apos;s/127.0.0.1/0.0.0.0/g&apos; /etc/mysql/my.cnf 重启mysql服务 service mysql restart 创建相关数据库 mysql -uroot -p$MYSQL_PASS &lt;&lt;EOF CREATE DATABASE nova; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;$MYSQL_PASS&apos;; CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO &apos;glance&apos;@&apos;%&apos; IDENTIFIED BY &apos;$MYSQL_PASS&apos;; CREATE DATABASE keystone; GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;%&apos;IDENTIFIED BY &apos;$MYSQL_PASS&apos;; FLUSH PRIVILEGES; EOF KeystoneKeystone是Openstack的核心，所有的组件，都需要通过keystone进行认证和授权。 租户（tenant）用户密码&nbsp;adminadminpassword&nbsp;servicenovapassword&nbsp;&nbsp;glancepassword&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 安装apt-get install -y keystone python-keystone python-keystoneclient 配置编辑/etc/keystone/keystone.conf，需要修改，注意数据库的连接IP和token根据自己的情况进行填写 keystone的默认token是ADMIN，我这里修改成随机生成，查看novarc获得 默认是采用sqlite连接，我们需要改成mysql [DEFAULT] #bind_host = 0.0.0.0 public_port = 5000 admin_port = 35357 #admin_token = ADMIN admin_token = a23debc939158820de0f [database] #connection = sqlite:////var/lib/keystone/keystone.db connection = mysql://keystone:password@10.210.0.94/keystone重启服务 service keystone restart同步keystone数据库 keystone-manage db_sync keystone的数据库，需要导入数据和endpoint，你可以一步一步用命令行导入，可以参考keystone白皮书 http://www.canonical.com/about-canonical/resources/white-papers/configuring-keystone-openstack-essex 为了方便，你可以直接使用下面2个脚本来进行全部的设置。 keystone_data.sh导入用户信息 endpoints.sh设置endpoint Keystone Datawget http://www.chenshake.com/wp-content/uploads/2012/07/keystone_data.sh_.txt mv keystone_data.sh_.txt keystone_data.sh bash keystone_data.sh 没任何输出，就表示正确，可以通过下面命令检查 echo $? 显示0，就表示脚本正确运行，千万不要重复运行脚本。 Endpoint 导入wget http://www.chenshake.com/wp-content/uploads/2012/07/endpoints.sh_.txt mv endpoints.sh_.txt endpoints.sh bash endpoints.sh 需要注意的是，这个脚本是假设你的glance服务和swift都是安装相同的服务器，如果你的glance在不同的服务器，你需要调整一下endpoint，可以在数据库里调整。 测试可以使用curl命令来测试。 命令如下 curl -d &apos;{&quot;auth&quot;: {&quot;tenantName&quot;: &quot;admin&quot;, &quot;passwordCredentials&quot;:{&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;password&quot;}}}&apos; -H &quot;Content-type:application/json&quot; http://$MASTER:35357/v2.0/tokens | python -mjson.tool 你就可以获得一个24小时的token。（注意，上面的脚本没有创建demo用户，所以没法用demo账号去测试） &quot;token&quot;: { &quot;expires&quot;: &quot;2012-09-27T02:09:37Z&quot;, &quot;id&quot;: &quot;c719448800214e189da04772c2f75e23&quot;, &quot;tenant&quot;: { &quot;description&quot;: null, &quot;enabled&quot;: true, &quot;id&quot;: &quot;dc7ca2e51139457dada2d0f7a3719476&quot;, &quot;name&quot;: &quot;admin&quot; } 通过下面命令，可以检查keystone的设置是否正确。 root@node:~# keystone user-list +----------------------------------+---------+----------------------+--------+ | id | enabled | email | name | +----------------------------------+---------+----------------------+--------+ | 1189d15892d24e00827e707bd2b7ab07 | True | admin@chenshake.com | admin | | cca4a4ed1e8842db99239dc98fb1617f | True | glance@chenshake.com | glance | | daccc34eacc7493989cd13df93e7f6bc | True | swift@chenshake.com | swift | | ee57b02c535d44f48943de13831da232 | True | nova@chenshake.com | nova | +----------------------------------+---------+----------------------+--------+ root@node17:~# keystone endpoint-list +----------------------------------+-----------+-----------------------------------------------+-----------------------------------------------+------------------------------------------+ | id | region | publicurl | internalurl | adminurl | +----------------------------------+-----------+-----------------------------------------------+-----------------------------------------------+------------------------------------------+ | 0b04e1baac1a4c9fb07490e0911192cf | RegionOne | http://10.1.199.17:5000/v2.0 | http://10.1.199.17:5000/v2.0 | http://10.1.199.17:35357/v2.0 | | 0d3315627d52419fa08095f9def5d7e4 | RegionOne | http://10.1.199.17:8776/v1/%(tenant_id)s | http://10.1.199.17:8776/v1/%(tenant_id)s | http://10.1.199.17:8776/v1/%(tenant_id)s | | 1c92290cba9f4a278b42dbdf2802096c | RegionOne | http://10.1.199.17:9292/v1 | http://10.1.199.17:9292/v1 | http://10.1.199.17:9292/v1 | | 56fe83ce20f341d99fc576770c275586 | RegionOne | http://10.1.199.17:8774/v2/%(tenant_id)s | http://10.1.199.17:8774/v2/%(tenant_id)s | http://10.1.199.17:8774/v2/%(tenant_id)s | | 5fb51aae00684e56818869918f86b564 | RegionOne | http://10.1.199.17:8080/v1/AUTH_%(tenant_id)s | http://10.1.199.17:8080/v1/AUTH_%(tenant_id)s | http://10.1.199.17:8080/v1 | | aaac7663872d493b85d9e583329be9ed | RegionOne | http://10.1.199.17:8773/services/Cloud | http://10.1.199.17:8773/services/Cloud | http://10.1.199.17:8773/services/Admin | +----------------------------------+-----------+-----------------------------------------------+-----------------------------------------------+------------------------------------------+ 可以使用下面命令来查看结果 keystone tenant-list keystone user-list keystone role-list Glance安装apt-get install -y glance glance-api glance-client glance-common glance-registry python-glance 配置编辑 /etc/glance/glance-api-paste.ini ，修改文档最后3行 #admin_tenant_name = %SERVICE_TENANT_NAME% #admin_user = %SERVICE_USER% #admin_password = %SERVICE_PASSWORD% admin_tenant_name = service admin_user = glance admin_password = password 编辑 /etc/glance/glance-registry.conf 和 /etc/glance/glance-api.conf ，改成使用mysql验证，作如下修改，ip地址根据自己情况设置。 [database] #sql_connection = sqlite:////var/lib/glance/glance.sqlite sql_connection = mysql://glance:password@10.210.0.94/glance [keystone_authtoken] auth_host = 10.210.0.94 auth_port = 35357 auth_protocol = http admin_tenant_name = service admin_user = glance admin_password = password [paste_deploy] flavor=keystone 重启glance服务 service glance-api restart &amp;&amp; service glance-registry restart 同步glance数据库 # glance-manage version_control 0 # glance-manage db_sync /usr/lib/python2.7/dist-packages/glance/registry/db/migrate_repo/versions/003_add_disk_format.py:47: SADeprecationWarning: useexisting is deprecated. Use extend_existing. useexisting=True) 重启glance服务 service glance-api restart &amp;&amp; service glance-registry restart 测试glance index 没有输出，表示正常，因为目前还没有镜像。 下载镜像我们下载CirrOS的image作为测试使用，只有10M。如果是ubuntu官方的image，220M，并且ubuntu官方的image，都是需要使用密钥登陆。 CirrOS下载并上传image wget https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img glance add name=cirros-0.3.0-x86_64 is_public=true container_format=bare \\ disk_format=qcow2 &lt; /root/cirros-0.3.0-x86_64-disk.img Cirros，可以使用用户名和密码登陆，也可以使用密钥登陆 user:cirros \\password:cubswin:) Ubuntu官方image下载并上传image wget http://cloud-images.ubuntu.com/precise/current/precise-server-cloudimg-amd64-disk1.img glance add name=&quot;Ubuntu 12.04 cloudimg amd64&quot; is_public=true container_format=ovf \\ disk_format=qcow2 &lt; /root/precise-server-cloudimg-amd64-disk1.img user：ubuntu 只能使用密钥登陆。 查看image# glance index ID Name Disk Format Container Format Size ------------------------------------ ------------------------------ -------------------- -------------------- -------------- 5dcf84a0-b491-4710-8d7a-5531bce0dedc cirros-0.3.0-x86_64 qcow2 bare 9761280 f4f62d8a-3e5b-4136-8547-ce3cb79771aa Ubuntu 12.04 cloudimg amd64 qcow2 ovf 230817792 Nova安装apt-get install -y nova-api nova-cert nova-common nova-objectstore \\ nova-scheduler nova-volume nova-consoleauth novnc python-nova python-novaclient \\ nova-compute nova-compute-kvm nova-network 如果你希望控制节点，不打算跑计算服务，装完后，把nova compute的服务停掉也可以。 配置编辑 /etc/nova/nova.conf 文件， 下面是我的nova.conf 文件的配置。 为了简单，大家直接copy下面内容，运行就可以，注意修改ip。 如果你是在虚拟机里安装，你需要把libvirt_type=kvm 改成 ibvirt_type=qemu [DEFAULT] dhcpbridge_flagfile=/etc/nova/nova.conf dhcpbridge=/usr/bin/nova-dhcpbridge logdir=/var/log/nova state_path=/var/lib/nova lock_path=/var/lock/nova force_dhcp_release=True iscsi_helper=tgtadm libvirt_use_virtio_for_bridges=True connection_type=libvirt root_helper=sudo nova-rootwrap /etc/nova/rootwrap.conf verbose=True ec2_private_dns_show_ip=True api_paste_config=/etc/nova/api-paste.ini volumes_path=/var/lib/nova/volumes enabled_apis=ec2,osapi_compute,metadata rpc_backend = rabbit rabbit_host = 10.210.0.94 my_ip = 10.210.0.94 vncserver_listen = 10.210.0.94 vncserver_proxyclient_address = 10.210.0.94 auth_strategy = keystone [keystone_authtoken] auth_uri = http://10.210.0.94:5000 auth_host = 10.210.0.94 auth_port = 35357 auth_protocol = http admin_tenant_name = service admin_user = nova admin_password = password [database] connection = mysql://nova:password@10.210.0.94/nova 设置目录权限 chown -R nova:nova /etc/nova 重启所有服务 service rabbitmq-server restart service libvirt-bin restart service nova-scheduler restart service nova-network restart service nova-cert restart service nova-compute restart service nova-api restart service nova-objectstore restart service nova-volume restart 也可以使用下面脚本进行重启 #!/bin/bash for a in rabbitmq-server libvirt-bin nova-network nova-cert nova-compute \\ nova-api nova-objectstore nova-scheduler nova-volume \\ novnc nova-consoleauth; do service &quot;$a&quot; stop; done for a in rabbitmq-server libvirt-bin nova-network nova-cert nova-compute \\ nova-api nova-objectstore nova-scheduler nova-volume \\ novnc nova-consoleauth; do service &quot;$a&quot; start; done 运行脚本 bash restart.sh Stopping rabbitmq-server: rabbitmq-server. libvirt-bin stop/waiting nova-network stop/waiting nova-cert stop/waiting nova-compute stop/waiting nova-api stop/waiting nova-objectstore stop/waiting nova-scheduler stop/waiting nova-volume stop/waiting * Stopping OpenStack NoVNC proxy nova-novncproxy [ OK ] nova-consoleauth stop/waiting Starting rabbitmq-server: SUCCESS rabbitmq-server. libvirt-bin start/running, process 9683 nova-network start/running, process 9703 nova-cert start/running, process 9713 nova-compute start/running, process 9724 nova-api start/running, process 9734 nova-objectstore start/running, process 9744 nova-scheduler start/running, process 9759 nova-volume start/running, process 9775 * Starting OpenStack NoVNC proxy nova-novncproxy [ OK ] nova-consoleauth start/running, process 9839 同步数据库 nova-manage db sync 会有一堆的输出，不过应该是没问题的。nova数据库里已经有相应的表，就表示正确。 # nova-manage db sync 2012-07-19 18:43:34 WARNING nova.utils [-] /usr/lib/python2.7/dist-packages/sqlalchemy/pool.py:639: SADeprecationWarning: The &apos;listeners&apos; argument to Pool (and create_engine()) is deprecated. Use event.listen(). Pool.__init__(self, creator, **kw) 2012-07-19 18:43:34 WARNING nova.utils [-] /usr/lib/python2.7/dist-packages/sqlalchemy/pool.py:145: SADeprecationWarning: Pool.add_listener is deprecated. Use event.listen() self.add_listener(l) 2012-07-19 18:43:34 AUDIT nova.db.sqlalchemy.fix_dns_domains [-] Applying database fix for Essex dns_domains table. 创建fix IPFIX IP，就是分配给虚拟机的实际IP地址。这些数据都会写入数据库。$FIXED_RANGE 在novarc里设置。 nova-manage network create private --fixed_range_v4=$FIXED_RANGE \\ --num_networks=1 --bridge=br100 --bridge_interface=eth1 \\ --network_size=256 --multi_host=T 创建floating IP所谓Floating IP，是亚马逊EC2的定义。简单说，就是公网的IP。他其实是通过类似防火墙类似，做一个映射。实际上是通过iptables来实现映射. nova-manage floating create --ip_range=$FLOATING_RANGE 重启nova服务 bash restart.sh libvirt-bin stop/waiting nova-network stop/waiting nova-cert stop/waiting nova-compute stop/waiting nova-api stop/waiting nova-objectstore stop/waiting nova-scheduler stop/waiting nova-volume stop/waiting * Stopping OpenStack NoVNC proxy nova-novncproxy [ OK ] nova-consoleauth stop/waiting libvirt-bin start/running, process 23232 nova-network start/running, process 23252 nova-cert start/running, process 23262 nova-compute start/running, process 23273 nova-api start/running, process 23285 nova-objectstore start/running, process 23303 nova-scheduler start/running, process 23321 nova-volume start/running, process 23336 * Starting OpenStack NoVNC proxy nova-novncproxy [ OK ] nova-consoleauth start/running, process 23386 测试可以尝试用下面命令去检查nova的状况 nova-manage service list 若state均为:-） \\说明安装成功 命令行创建虚拟机的过程nova keypair-add oskey &gt; oskey.priv chmod 600 oskey.priv nova flavor-list nova image-list nova boot --flavor 2 --key_name oskey --image ea3ffba1-065e-483f-bfe2-c84184ee76be test1 nova secgroup-add-rule default tcp 22 22 0.0.0.0/0 nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0 这个时候，你在服务器上可以直接ssh到虚拟机上，ubuntu的虚拟机，用户是ubuntu。虚拟机的Ip # nova list +--------------------------------------+-------+--------+------------------+ | ID | Name | Status | Networks | +--------------------------------------+-------+--------+------------------+ | 61e93d62-c926-46fa-8e0c-48073b7e58b0 | test1 | ACTIVE | private=10.0.0.2 | | 6976e539-32d9-48a6-9fb5-28a3cdb55f71 | test2 | ACTIVE | private=10.0.0.4 | +--------------------------------------+-------+--------+------------------+ 在服务器上直接ssh到虚拟机，如果你在远程，就需要分配floating IP。 ssh -i oskey.priv ubuntu@10.0.0.4 登陆虚拟机后，你可以查看一下路由 $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default 10.0.0.3 0.0.0.0 UG 100 0 0 eth0 10.0.0.0 * 255.255.255.0 U 0 0 0 eth0 显示网关是10.0.0.3，这个时候，你看一下 root@node:~# ifconfig br100 Link encap:Ethernet HWaddr 00:e0:81:d8:4a:23 inet addr:10.0.0.3 Bcast:10.0.0.255 Mask:255.255.255.0 inet6 addr: fe80::ccfc:5aff:fef5:4345/64 Scope:Link 需要注意的是：br100的IP，需要你创建第一个虚拟机，他才会获得IP。 Dashobard安装apt-get install -y apache2 libapache2-mod-wsgi openstack-dashboard 重启nova api restart nova-api 这个时候，就可以访问dashboard。 测试登陆dashobard，下面地址是我的horizon界面，可以直接访问，注：需要北航内网。 http://10.210.0.94/horizon \\user:admin \\pass:password","categories":[],"tags":[{"name":"OpenStack","slug":"OpenStack","permalink":"http://yoursite.com/tags/OpenStack/"}]},{"title":"使用kubeadm搭建kubernetes集群","slug":"2_使用kubeadm搭建kubernetes集群","date":"2018-10-08T14:29:05.109Z","updated":"2018-09-16T05:08:21.395Z","comments":true,"path":"2018/10/08/2_使用kubeadm搭建kubernetes集群/","link":"","permalink":"http://yoursite.com/2018/10/08/2_使用kubeadm搭建kubernetes集群/","excerpt":"","text":"使用kubeadm搭建kubernetes集群参考kubernetes官方文档https://kubernetes.io/docs/setup 作者：薛世卿如遇安装问题，可以咨询我。 注意：本文安装过程为在线安装，需要科学上网 一、网络配置使用shadowsocks配置本地代理后，服务器可以访问外网。与通常的shadowsocks（下简称ss）客户端配置不同，Linux系统还需要polipo完成socks 5协议到HTTP协议的转换。 安装shadowsocks由于ss是基于Python开发，我们首先需要安装Python。Python2和Python3皆可，但需要注意Py2和3的版本冲突问题，不要重复安装。 确定系统中无Python后 sudo apt-get install python 安装包管理应用pip（Py3安装pip3） sudo apt-get install python-pip 安装完毕之后，通过pip直接安装ss sudo pip install shadowsocks 配置shadowsocks客户端新建一个配置文件client.json，配置相应参数 { &quot;server&quot;:&quot;{your-ss-server-ip}&quot;, &quot;server_port&quot;:{your-ss-server-port}, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;{your-password}&quot;, &quot;timeout&quot;:600, &quot;method&quot;:&quot;aes-256-cfb&quot; } 启动ss客户端 sudo sslocal -c client.json -d start 如何配置ss服务器端不在本文讨论范围之内。 配置全局代理我们需要polipo完成socks 5和HTTP协议间的转换。首先是安装polipo。 sudo apt-get install polipo 修改polipo的配置文件/etc/polipo/config logSyslog = true logFile = /var/log/polipo/polipo.log proxyAddress = &quot;0.0.0.0&quot; socksParentProxy = &quot;127.0.0.1:1080&quot; socksProxyType = socks5 chunkHighMark = 50331648 objectHighMark = 16384 serverMaxSlots = 64 serverSlots = 16 serverSlots1 = 32 重启polipo sudo /etc/init.d/polipo restart 为终端配置http代理（只对当前终端生效的临时环境变量） export http_proxy=&quot;http://127.0.0.1:8123/&quot; 此时为全局代理模式，如果需要关闭代理，删除环境变量即可，命令如下 unset http_proxy 测试代理 curl www.google.com 重启服务器后，通常需要重新执行下面的指令来使用代理 sudo sslocal -c client.json -d start export http_proxy=&quot;http://127.0.0.1:8123/&quot; 此时通过Debian的apt-get命令就可以访问docker.io和kubernetes.io，畅通地完成以下安装过程。 二、安装kubeadm本章节介绍了如何通过kubeadm搭建集群 准备工作 关闭防火墙规则 关闭交换分区 确认端口占用 首先，查看防火墙规则 iptables -L 如果有可能影响到节点通信的规则，可以通过以下命令放开iptables规则（注意，此操作可能会导致ssh连接永久掉线，请慎重操作,为了稳妥起见可以只执行后三条命令） iptables -F //清除规则链中已有条目 iptables -X //清除自定义规则 iptables -Z //清空数据表计算器和字节计数器 iptables -P INPUT ACCEPT iptables -P OUTPUT ACCEPT iptables -P FORWARD ACCEPT iptables-save 然后，关闭各个节点上的swap分区 swapoff -a 注释掉swap分区 vi /etc/fstab #/dev/mapper/c1-swap swap swap defaults 0 0 kubernetes服务端口占用情况如下 Master node(s) Protocol Direction Port Range Purpose Used By TCP Inbound 6443* Kubernetes API server All TCP Inbound 2379-2380 etcd server client API kube-apiserver, etcd TCP Inbound 10250 Kubelet API Self, Control plane TCP Inbound 10251 kube-scheduler Self TCP Inbound 10252 kube-controller-manager Self Worker node(s) Protocol Direction Port Range Purpose Used By TCP Inbound 10250 Kubelet API Self, Control plane TCP Inbound 30000-32767 NodePort Services** All ** Default port range for NodePort Services. Any port numbers marked with * are overridable, so you will need to ensure any custom ports you provide are also open. Although etcd ports are included in master nodes, you can also host your own etcd cluster externally or on custom ports. The pod network plugin you use (see below) may also require certain ports to be open. Since this differs with each pod network plugin, please see the documentation for the plugins about what port(s) those need. 安装docker请确保http代理已设置，即已经设置了$http_proxy 安装Docker CE 17.03 apt-get update apt-get install -y apt-transport-https ca-certificates curl software-properties-common curl -fsSL http://download.docker.com/linux/ubuntu/gpg | apt-key add - add-apt-repository &quot;deb http://download.docker.com/linux/$(. /etc/os-release; echo &quot;$ID&quot;) $(lsb_release -cs) stable&quot; apt-get update &amp;&amp; apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep 17.03 | head -1 | awk &apos;{print $3}&apos;) 安装kubeadm，kubelet和kubectl你将会在机器上安装如下组件 kubeadm： the command to bootstrap the cluster. kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers. kubectl: the command line util to talk to your cluster. apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s http://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet=1.10.4-00 kubeadm=1.10.4-00 kubectl=1.10.4-00 kubernetes-cni apt-mark hold kubelet kubeadm kubectl kubernetes-cni 设置docker代理为了避免docker拉取镜像时发生网络错误，进行如下设置。 vi /etc/default/docker 添加以下内容 HTTP_PROXY=&quot;http://127.0.0.1:8123/&quot; HTTPS_PROXY=&quot;https://127.0.0.1:8123/&quot; export HTTP_PROXY HTTPS_PROXY 编辑docker.server EnvironmentFile=-/etc/default/docker ExecStart=/usr/bin/docker daemon -H fd:// $DOCKER_OPTS 重启docker systemctl daemon-reload systemctl restart docker.service 主节点初始化在主节点上，执行如下命令 kubeadm init --kubernetes-version=1.10.4-00 --pod-network-cidr=10.244.0.0/16 如果执行失败，可以查看kubelet的运行状况，一般是由于kubelet运行失败导致的。 systemctl status kubelet 大致显示如下内容 [init] Using Kubernetes version: v1.10.4 [init] Using Authorization modes: [Node RBAC] [preflight] Running pre-flight checks. [WARNING FileExisting-crictl]: crictl not found in system path [certificates] Using the existing ca certificate and key. [certificates] Using the existing apiserver certificate and key. [certificates] Using the existing apiserver-kubelet-client certificate and key. [certificates] Using the existing sa key. [certificates] Using the existing front-proxy-ca certificate and key. [certificates] Using the existing front-proxy-client certificate and key. [certificates] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot; [kubeconfig] Using existing up-to-date KubeConfig file: &quot;admin.conf&quot; [kubeconfig] Using existing up-to-date KubeConfig file: &quot;kubelet.conf&quot; [kubeconfig] Using existing up-to-date KubeConfig file: &quot;controller-manager.conf&quot; [kubeconfig] Using existing up-to-date KubeConfig file: &quot;scheduler.conf&quot; [controlplane] Wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot; [controlplane] Wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot; [controlplane] Wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot; [etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot; [init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;. [init] This might take a minute or longer if the control plane images have to be pulled. [apiclient] All control plane components are healthy after 27.003370 seconds [uploadconfig] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace [markmaster] Will mark node master1 as master by adding a label and a taint [markmaster] Master master1 tainted and labelled with key/value: node-role.kubernetes.io/master=&quot;&quot; [bootstraptoken] Using token: d405c1.18b51150e22ffe72 [bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace [addons] Applied essential addon: kube-dns [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join --token d405c1.18b51150e22ffe72 192.168.128.26:6443 --discovery-token-ca-cert-hash sha256:936229f8381de8df72e8b0de8a349a0099f0d0fc0407ca17a5bffe2e6 请记录下最后一句话，它将是之后加入节点的指令（非常重要），之后执行下面的命令 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 对于root用户 export KUBECONFIG=/etc/kubernetes/admin.conf 也可以直接放到~/.bash_profile echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; ~/.bash_profile 修改网桥设置 sysctl net.bridge.bridge-nf-call-iptables=1 安装网络组件Flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml 查看节点 kubectl get nodes 此时master节点不作为工作节点，如果你希望pods也能够调度到master节点上，执行以下命令 kubectl taint nodes --all node-role.kubernetes.io/master- 添加node节点首先网络设置同master节点，并按同样的方式完成kubeadm，kubelet,kubernetes-cni的安装，保证各节点可以互相通信。 进入root用户，运行刚才记录下来的kubeadm join …命令 kubeadm join --token d405c1.18b51150e22ffe72 192.168.128.26:6443 --discovery-token-ca-cert-hash sha256:936229f8381de8df72e8b0de8a349a0099f0d0fc0407ca17a5bffe2e6 如果运行失败，在下次运行前，需要停止kubelet服务对端口的占用，同时按照提示删除生成的一些文件，如etcd文件夹下面的一些内容，重新运行即可。 运行成功后，通过以下命令可以看到加入的节点 kubectl get nodes 三、配置DashBoard由于新版本的kubernetes加入了rbac，非本机访问DashBoard会遇到权限上的问题。 我们采用的方法是直接创建admin用户无视认证过程，不然就需要配置证书，过程颇为复杂。 创建dashboard-admin.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard labels: k8s-app: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system 安装DashBoard kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 创建admin用户 kubectl apply -f dashboard-admin.yaml master节点运行kubectl代理 kubectl proxy --address=&apos;{master-ip}&apos; --disable-filter=true 启动后，访问以下地址 http://{your-master-ip}:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ 跳过登录即可","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]}]}