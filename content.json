{"meta":{"title":"Xue's Homepage","subtitle":null,"description":"A Computer Science student in Dalian University of Technology","author":"Shiqing Xue","url":"http://yoursite.com"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2018-09-15T09:17:06.689Z","updated":"2018-05-13T15:33:24.412Z","comments":false,"path":"/404.html","permalink":"http://yoursite.com//404.html","excerpt":"","text":""},{"title":"书单","date":"2018-09-15T09:17:06.689Z","updated":"2018-05-13T15:33:24.412Z","comments":false,"path":"books/index.html","permalink":"http://yoursite.com/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2018-09-15T09:17:06.689Z","updated":"2018-05-13T15:33:24.412Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2018-09-15T09:17:06.689Z","updated":"2018-05-13T15:33:24.412Z","comments":true,"path":"links/index.html","permalink":"http://yoursite.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2018-09-15T09:17:06.693Z","updated":"2018-05-13T15:33:24.412Z","comments":false,"path":"repository/index.html","permalink":"http://yoursite.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-09-15T09:17:06.693Z","updated":"2018-05-13T15:33:24.412Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"使用kubeadm搭建kubernetes集群","slug":"2_使用kubeadm搭建kubernetes集群","date":"2018-09-15T14:35:03.517Z","updated":"2018-09-15T14:22:19.861Z","comments":true,"path":"2018/09/15/2_使用kubeadm搭建kubernetes集群/","link":"","permalink":"http://yoursite.com/2018/09/15/2_使用kubeadm搭建kubernetes集群/","excerpt":"","text":"使用kubeadm搭建kubernetes集群参考kubernetes官方文档https://kubernetes.io/docs/setup 作者：薛世卿如遇安装问题，可以咨询我。 注意：本文安装过程为在线安装，需要科学上网 一、网络配置使用shadowsocks配置本地代理后，服务器可以访问外网。与通常的shadowsocks（下简称ss）客户端配置不同，Linux系统还需要polipo完成socks 5协议到HTTP协议的转换。 安装shadowsocks由于ss是基于Python开发，我们首先需要安装Python。Python2和Python3皆可，但需要注意Py2和3的版本冲突问题，不要重复安装。 确定系统中无Python后 sudo apt-get install python 安装包管理应用pip（Py3安装pip3） sudo apt-get install python-pip 安装完毕之后，通过pip直接安装ss sudo pip install shadowsocks 配置shadowsocks客户端新建一个配置文件client.json，配置相应参数 { &quot;server&quot;:&quot;{your-ss-server-ip}&quot;, &quot;server_port&quot;:{your-ss-server-port}, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;{your-password}&quot;, &quot;timeout&quot;:600, &quot;method&quot;:&quot;aes-256-cfb&quot; } 启动ss客户端 sudo sslocal -c client.json -d start 如何配置ss服务器端不在本文讨论范围之内。 配置全局代理我们需要polipo完成socks 5和HTTP协议间的转换。首先是安装polipo。 sudo apt-get install polipo 修改polipo的配置文件/etc/polipo/config logSyslog = true logFile = /var/log/polipo/polipo.log proxyAddress = &quot;0.0.0.0&quot; socksParentProxy = &quot;127.0.0.1:1080&quot; socksProxyType = socks5 chunkHighMark = 50331648 objectHighMark = 16384 serverMaxSlots = 64 serverSlots = 16 serverSlots1 = 32 重启polipo sudo /etc/init.d/polipo restart 为终端配置http代理（只对当前终端生效的临时环境变量） export http_proxy=&quot;http://127.0.0.1:8123/&quot; 此时为全局代理模式，如果需要关闭代理，删除环境变量即可，命令如下 unset http_proxy 测试代理 curl www.google.com 重启服务器后，通常需要重新执行下面的指令来使用代理 sudo sslocal -c client.json -d start export http_proxy=&quot;http://127.0.0.1:8123/&quot; 此时通过Debian的apt-get命令就可以访问docker.io和kubernetes.io，畅通地完成以下安装过程。 二、安装kubeadm本章节介绍了如何通过kubeadm搭建集群 准备工作 关闭防火墙规则 关闭交换分区 确认端口占用 首先，查看防火墙规则 iptables -L 如果有可能影响到节点通信的规则，可以通过以下命令放开iptables规则（注意，此操作可能会导致ssh连接永久掉线，请慎重操作,为了稳妥起见可以只执行后三条命令） iptables -F //清除规则链中已有条目 iptables -X //清除自定义规则 iptables -Z //清空数据表计算器和字节计数器 iptables -P INPUT ACCEPT iptables -P OUTPUT ACCEPT iptables -P FORWARD ACCEPT iptables-save 然后，关闭各个节点上的swap分区 swapoff -a 注释掉swap分区 vi /etc/fstab #/dev/mapper/c1-swap swap swap defaults 0 0 kubernetes服务端口占用情况如下 Master node(s)ProtocolDirectionPort RangePurposeUsed ByTCPInbound6443*Kubernetes API serverAllTCPInbound2379-2380etcd server client APIkube-apiserver, etcdTCPInbound10250Kubelet APISelf, Control planeTCPInbound10251kube-schedulerSelfTCPInbound10252kube-controller-managerSelf Worker node(s)ProtocolDirectionPort RangePurposeUsed ByTCPInbound10250Kubelet APISelf, Control planeTCPInbound30000-32767NodePort Services**All ** Default port range for NodePort Services. Any port numbers marked with * are overridable, so you will need to ensure any custom ports you provide are also open. Although etcd ports are included in master nodes, you can also host your own etcd cluster externally or on custom ports. The pod network plugin you use (see below) may also require certain ports to be open. Since this differs with each pod network plugin, please see the documentation for the plugins about what port(s) those need. 安装docker请确保http代理已设置，即已经设置了$http_proxy 安装Docker CE 17.03 apt-get update apt-get install -y apt-transport-https ca-certificates curl software-properties-common curl -fsSL http://download.docker.com/linux/ubuntu/gpg | apt-key add - add-apt-repository &quot;deb http://download.docker.com/linux/$(. /etc/os-release; echo &quot;$ID&quot;) $(lsb_release -cs) stable&quot; apt-get update &amp;&amp; apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep 17.03 | head -1 | awk &apos;{print $3}&apos;) 安装kubeadm，kubelet和kubectl你将会在机器上安装如下组件 kubeadm： the command to bootstrap the cluster. kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers. kubectl: the command line util to talk to your cluster. apt-get update &amp;&amp; apt-get install -y apt-transport-https curl curl -s http://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet=1.10.4-00 kubeadm=1.10.4-00 kubectl=1.10.4-00 kubernetes-cni apt-mark hold kubelet kubeadm kubectl kubernetes-cni 设置docker代理为了避免docker拉取镜像时发生网络错误，进行如下设置。 vi /etc/default/docker 添加以下内容 HTTP_PROXY=&quot;http://127.0.0.1:8123/&quot; HTTPS_PROXY=&quot;https://127.0.0.1:8123/&quot; export HTTP_PROXY HTTPS_PROXY 编辑docker.server EnvironmentFile=-/etc/default/docker ExecStart=/usr/bin/docker daemon -H fd:// $DOCKER_OPTS 重启docker systemctl daemon-reload systemctl restart docker.service 主节点初始化在主节点上，执行如下命令 kubeadm init --kubernetes-version=1.10.4-00 --pod-network-cidr=10.244.0.0/16 如果执行失败，可以查看kubelet的运行状况，一般是由于kubelet运行失败导致的。 systemctl status kubelet 大致显示如下内容 [init] Using Kubernetes version: v1.10.4 [init] Using Authorization modes: [Node RBAC] [preflight] Running pre-flight checks. [WARNING FileExisting-crictl]: crictl not found in system path [certificates] Using the existing ca certificate and key. [certificates] Using the existing apiserver certificate and key. [certificates] Using the existing apiserver-kubelet-client certificate and key. [certificates] Using the existing sa key. [certificates] Using the existing front-proxy-ca certificate and key. [certificates] Using the existing front-proxy-client certificate and key. [certificates] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot; [kubeconfig] Using existing up-to-date KubeConfig file: &quot;admin.conf&quot; [kubeconfig] Using existing up-to-date KubeConfig file: &quot;kubelet.conf&quot; [kubeconfig] Using existing up-to-date KubeConfig file: &quot;controller-manager.conf&quot; [kubeconfig] Using existing up-to-date KubeConfig file: &quot;scheduler.conf&quot; [controlplane] Wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot; [controlplane] Wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot; [controlplane] Wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot; [etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot; [init] Waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;. [init] This might take a minute or longer if the control plane images have to be pulled. [apiclient] All control plane components are healthy after 27.003370 seconds [uploadconfig] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace [markmaster] Will mark node master1 as master by adding a label and a taint [markmaster] Master master1 tainted and labelled with key/value: node-role.kubernetes.io/master=&quot;&quot; [bootstraptoken] Using token: d405c1.18b51150e22ffe72 [bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace [addons] Applied essential addon: kube-dns [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join --token d405c1.18b51150e22ffe72 192.168.128.26:6443 --discovery-token-ca-cert-hash sha256:936229f8381de8df72e8b0de8a349a0099f0d0fc0407ca17a5bffe2e6 请记录下最后一句话，它将是之后加入节点的指令（非常重要），之后执行下面的命令 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 对于root用户 export KUBECONFIG=/etc/kubernetes/admin.conf 也可以直接放到~/.bash_profile echo &quot;export KUBECONFIG=/etc/kubernetes/admin.conf&quot; &gt;&gt; ~/.bash_profile 修改网桥设置 sysctl net.bridge.bridge-nf-call-iptables=1 安装网络组件Flannel kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml 查看节点 kubectl get nodes 此时master节点不作为工作节点，如果你希望pods也能够调度到master节点上，执行以下命令 kubectl taint nodes --all node-role.kubernetes.io/master- 添加node节点首先网络设置同master节点，并按同样的方式完成kubeadm，kubelet,kubernetes-cni的安装，保证各节点可以互相通信。 进入root用户，运行刚才记录下来的kubeadm join …命令 kubeadm join --token d405c1.18b51150e22ffe72 192.168.128.26:6443 --discovery-token-ca-cert-hash sha256:936229f8381de8df72e8b0de8a349a0099f0d0fc0407ca17a5bffe2e6 如果运行失败，在下次运行前，需要停止kubelet服务对端口的占用，同时按照提示删除生成的一些文件，如etcd文件夹下面的一些内容，重新运行即可。 运行成功后，通过以下命令可以看到加入的节点 kubectl get nodes 三、配置DashBoard由于新版本的kubernetes加入了rbac，非本机访问DashBoard会遇到权限上的问题。 我们采用的方法是直接创建admin用户无视认证过程，不然就需要配置证书，过程颇为复杂。 创建dashboard-admin.yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard labels: k8s-app: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system 安装DashBoard kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 创建admin用户 kubectl apply -f dashboard-admin.yaml master节点运行kubectl代理 kubectl proxy --address=&apos;{master-ip}&apos; --disable-filter=true 启动后，访问以下地址 http://{your-master-ip}:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ 跳过登录即可","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://yoursite.com/tags/kubernetes/"}]},{"title":"OpenStack单节点安装","slug":"1_OpenStack单节点安装","date":"2018-06-02T05:25:44.115Z","updated":"2018-06-02T05:25:44.115Z","comments":true,"path":"2018/06/02/1_OpenStack单节点安装/","link":"","permalink":"http://yoursite.com/2018/06/02/1_OpenStack单节点安装/","excerpt":"","text":"OpenStack单节点单网卡安装参考陈沙克日志http://www.chenshake.com/ubuntu-12-04-openstack-essex-installation-single-node/ 作者：薛世卿如遇安装问题，可以咨询我。 注意，本文档不包含陈沙克的可选安装部分，这部分内容包括 ntp服务器 ISCSI（供Nova-volume使用） Nova-volume 一、准备系统系统信息我安装的节点为 IVIC 云主机 IP为 10.210.0.94 开始安装只需确保系统安装ssh server即可，然后更新源 apt-get update &amp;&amp; apt-get -y dist-upgrade 设置网络编辑/etc/network/interfaces，IP地址和DNS根据实际情况调整。 auto lo iface lo inet loopback auto eth0 iface eth0 inet static address 10.210.0.94 netmask 255.255.0.0 gateway 10.210.0.1 dns-nameserver 202.112.128.51 auto eth0:0 iface eth0:0 inet manual up ifconfig eth0:0 up Bridge使用linux的bridge和iptables来实现OpenStack的网络配置 apt-get -y install bridge-utils 环境变量你可以根据你的实际情况修改admin的密码和mysql的密码。下面文档和数据库相关的密码都是相同，你只需要修改novarc就可以。 运行完下面的命令，你再对novarc进行修改。 cat &gt;/root/novarc &lt;&lt;EOF export OS_TENANT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=password export MYSQL_PASS=password export SERVICE_PASSWORD=password export FIXED_RANGE=10.0.0.0/24 export FLOATING_RANGE=$(/sbin/ifconfig eth0 | awk &apos;/inet addr/ {print $2}&apos; | cut -f2 -d &quot;:&quot; | awk -F &quot;.&quot; &apos;{print $1&quot;.&quot;$2&quot;.&quot;$3}&apos;).224/27 export OS_AUTH_URL=&quot;http://localhost:5000/v2.0/&quot; export SERVICE_ENDPOINT=&quot;http://localhost:35357/v2.0&quot; export SERVICE_TOKEN=$(openssl rand -hex 10) export MASTER=&quot;$(/sbin/ifconfig eth0 | awk &apos;/inet addr/ {print $2}&apos; | cut -f2 -d &quot;:&quot;)&quot; EOF 我的novarc内容，与原博客不同的是，出于某种原因我注释掉了两行，具体原因我也不是很清楚了2333。 ubuntu@mu:~$ sudo cat /root/novarc export OS_TENANT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=password export MYSQL_PASS=password export SERVICE_PASSWORD=password export FIXED_RANGE=10.0.0.0/24 export FLOATING_RANGE=10.210.0.94/27 export OS_AUTH_URL=&quot;http://localhost:5000/v2.0/&quot; #export SERVICE_ENDPOINT=&quot;http://localhost:35357/v2.0&quot; #export SERVICE_TOKEN=a23debc939158820de0f export MASTER=&quot;10.210.0.94&quot; 确认没有问题或者进行修改，运行 source novarc echo &quot;source novarc&quot;&gt;&gt;.bashrc MYSQL在Openstack组件里，Nova，Keystone, Glance, 都需要用到数据库。所以我们需要创建相关的数据库和用户。 应用数据库数据库用户密码mysqlrootpasswordnovanovapasswordglanceglancepasswordkeystonekeystonepassword&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 安装mysql自动安装 cat &lt;&lt;MYSQL_PRESEED | debconf-set-selections mysql-server-5.5 mysql-server/root_password password $MYSQL_PASS mysql-server-5.5 mysql-server/root_password_again password $MYSQL_PASS mysql-server-5.5 mysql-server/start_on_boot boolean true MYSQL_PRESEED Openstack都是Python写的，所以你需要python-mysqldb，安装过程，就不会提示你输入root密码 apt-get install -y mysql-server python-mysqldb 配置编辑/etc/mysql/my.cnf, 允许网络访问mysql #bind-address = 127.0.0.1 bind-address = 0.0.0.0 或者直接运行下面命令 sed -i &apos;s/127.0.0.1/0.0.0.0/g&apos; /etc/mysql/my.cnf 重启mysql服务 service mysql restart 创建相关数据库 mysql -uroot -p$MYSQL_PASS &lt;&lt;EOF CREATE DATABASE nova; GRANT ALL PRIVILEGES ON nova.* TO &apos;nova&apos;@&apos;%&apos; IDENTIFIED BY &apos;$MYSQL_PASS&apos;; CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO &apos;glance&apos;@&apos;%&apos; IDENTIFIED BY &apos;$MYSQL_PASS&apos;; CREATE DATABASE keystone; GRANT ALL PRIVILEGES ON keystone.* TO &apos;keystone&apos;@&apos;%&apos;IDENTIFIED BY &apos;$MYSQL_PASS&apos;; FLUSH PRIVILEGES; EOF KeystoneKeystone是Openstack的核心，所有的组件，都需要通过keystone进行认证和授权。 租户（tenant）用户密码&nbsp;adminadminpassword&nbsp;servicenovapassword&nbsp;&nbsp;glancepassword&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 安装apt-get install -y keystone python-keystone python-keystoneclient 配置编辑/etc/keystone/keystone.conf，需要修改，注意数据库的连接IP和token根据自己的情况进行填写 keystone的默认token是ADMIN，我这里修改成随机生成，查看novarc获得 默认是采用sqlite连接，我们需要改成mysql [DEFAULT] #bind_host = 0.0.0.0 public_port = 5000 admin_port = 35357 #admin_token = ADMIN admin_token = a23debc939158820de0f [database] #connection = sqlite:////var/lib/keystone/keystone.db connection = mysql://keystone:password@10.210.0.94/keystone重启服务 service keystone restart同步keystone数据库 keystone-manage db_sync keystone的数据库，需要导入数据和endpoint，你可以一步一步用命令行导入，可以参考keystone白皮书 http://www.canonical.com/about-canonical/resources/white-papers/configuring-keystone-openstack-essex 为了方便，你可以直接使用下面2个脚本来进行全部的设置。 keystone_data.sh导入用户信息 endpoints.sh设置endpoint Keystone Datawget http://www.chenshake.com/wp-content/uploads/2012/07/keystone_data.sh_.txt mv keystone_data.sh_.txt keystone_data.sh bash keystone_data.sh 没任何输出，就表示正确，可以通过下面命令检查 echo $? 显示0，就表示脚本正确运行，千万不要重复运行脚本。 Endpoint 导入wget http://www.chenshake.com/wp-content/uploads/2012/07/endpoints.sh_.txt mv endpoints.sh_.txt endpoints.sh bash endpoints.sh 需要注意的是，这个脚本是假设你的glance服务和swift都是安装相同的服务器，如果你的glance在不同的服务器，你需要调整一下endpoint，可以在数据库里调整。 测试可以使用curl命令来测试。 命令如下 curl -d &apos;{&quot;auth&quot;: {&quot;tenantName&quot;: &quot;admin&quot;, &quot;passwordCredentials&quot;:{&quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;password&quot;}}}&apos; -H &quot;Content-type:application/json&quot; http://$MASTER:35357/v2.0/tokens | python -mjson.tool 你就可以获得一个24小时的token。（注意，上面的脚本没有创建demo用户，所以没法用demo账号去测试） &quot;token&quot;: { &quot;expires&quot;: &quot;2012-09-27T02:09:37Z&quot;, &quot;id&quot;: &quot;c719448800214e189da04772c2f75e23&quot;, &quot;tenant&quot;: { &quot;description&quot;: null, &quot;enabled&quot;: true, &quot;id&quot;: &quot;dc7ca2e51139457dada2d0f7a3719476&quot;, &quot;name&quot;: &quot;admin&quot; } 通过下面命令，可以检查keystone的设置是否正确。 root@node:~# keystone user-list +----------------------------------+---------+----------------------+--------+ | id | enabled | email | name | +----------------------------------+---------+----------------------+--------+ | 1189d15892d24e00827e707bd2b7ab07 | True | admin@chenshake.com | admin | | cca4a4ed1e8842db99239dc98fb1617f | True | glance@chenshake.com | glance | | daccc34eacc7493989cd13df93e7f6bc | True | swift@chenshake.com | swift | | ee57b02c535d44f48943de13831da232 | True | nova@chenshake.com | nova | +----------------------------------+---------+----------------------+--------+ root@node17:~# keystone endpoint-list +----------------------------------+-----------+-----------------------------------------------+-----------------------------------------------+------------------------------------------+ | id | region | publicurl | internalurl | adminurl | +----------------------------------+-----------+-----------------------------------------------+-----------------------------------------------+------------------------------------------+ | 0b04e1baac1a4c9fb07490e0911192cf | RegionOne | http://10.1.199.17:5000/v2.0 | http://10.1.199.17:5000/v2.0 | http://10.1.199.17:35357/v2.0 | | 0d3315627d52419fa08095f9def5d7e4 | RegionOne | http://10.1.199.17:8776/v1/%(tenant_id)s | http://10.1.199.17:8776/v1/%(tenant_id)s | http://10.1.199.17:8776/v1/%(tenant_id)s | | 1c92290cba9f4a278b42dbdf2802096c | RegionOne | http://10.1.199.17:9292/v1 | http://10.1.199.17:9292/v1 | http://10.1.199.17:9292/v1 | | 56fe83ce20f341d99fc576770c275586 | RegionOne | http://10.1.199.17:8774/v2/%(tenant_id)s | http://10.1.199.17:8774/v2/%(tenant_id)s | http://10.1.199.17:8774/v2/%(tenant_id)s | | 5fb51aae00684e56818869918f86b564 | RegionOne | http://10.1.199.17:8080/v1/AUTH_%(tenant_id)s | http://10.1.199.17:8080/v1/AUTH_%(tenant_id)s | http://10.1.199.17:8080/v1 | | aaac7663872d493b85d9e583329be9ed | RegionOne | http://10.1.199.17:8773/services/Cloud | http://10.1.199.17:8773/services/Cloud | http://10.1.199.17:8773/services/Admin | +----------------------------------+-----------+-----------------------------------------------+-----------------------------------------------+------------------------------------------+ 可以使用下面命令来查看结果 keystone tenant-list keystone user-list keystone role-list Glance安装apt-get install -y glance glance-api glance-client glance-common glance-registry python-glance 配置编辑 /etc/glance/glance-api-paste.ini ，修改文档最后3行 #admin_tenant_name = %SERVICE_TENANT_NAME% #admin_user = %SERVICE_USER% #admin_password = %SERVICE_PASSWORD% admin_tenant_name = service admin_user = glance admin_password = password 编辑 /etc/glance/glance-registry.conf 和 /etc/glance/glance-api.conf ，改成使用mysql验证，作如下修改，ip地址根据自己情况设置。 [database] #sql_connection = sqlite:////var/lib/glance/glance.sqlite sql_connection = mysql://glance:password@10.210.0.94/glance [keystone_authtoken] auth_host = 10.210.0.94 auth_port = 35357 auth_protocol = http admin_tenant_name = service admin_user = glance admin_password = password [paste_deploy] flavor=keystone 重启glance服务 service glance-api restart &amp;&amp; service glance-registry restart 同步glance数据库 # glance-manage version_control 0 # glance-manage db_sync /usr/lib/python2.7/dist-packages/glance/registry/db/migrate_repo/versions/003_add_disk_format.py:47: SADeprecationWarning: useexisting is deprecated. Use extend_existing. useexisting=True) 重启glance服务 service glance-api restart &amp;&amp; service glance-registry restart 测试glance index 没有输出，表示正常，因为目前还没有镜像。 下载镜像我们下载CirrOS的image作为测试使用，只有10M。如果是ubuntu官方的image，220M，并且ubuntu官方的image，都是需要使用密钥登陆。 CirrOS下载并上传image wget https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img glance add name=cirros-0.3.0-x86_64 is_public=true container_format=bare \\ disk_format=qcow2 &lt; /root/cirros-0.3.0-x86_64-disk.img Cirros，可以使用用户名和密码登陆，也可以使用密钥登陆 user:cirros \\password:cubswin:) Ubuntu官方image下载并上传image wget http://cloud-images.ubuntu.com/precise/current/precise-server-cloudimg-amd64-disk1.img glance add name=&quot;Ubuntu 12.04 cloudimg amd64&quot; is_public=true container_format=ovf \\ disk_format=qcow2 &lt; /root/precise-server-cloudimg-amd64-disk1.img user：ubuntu 只能使用密钥登陆。 查看image# glance index ID Name Disk Format Container Format Size ------------------------------------ ------------------------------ -------------------- -------------------- -------------- 5dcf84a0-b491-4710-8d7a-5531bce0dedc cirros-0.3.0-x86_64 qcow2 bare 9761280 f4f62d8a-3e5b-4136-8547-ce3cb79771aa Ubuntu 12.04 cloudimg amd64 qcow2 ovf 230817792 Nova安装apt-get install -y nova-api nova-cert nova-common nova-objectstore \\ nova-scheduler nova-volume nova-consoleauth novnc python-nova python-novaclient \\ nova-compute nova-compute-kvm nova-network 如果你希望控制节点，不打算跑计算服务，装完后，把nova compute的服务停掉也可以。 配置编辑 /etc/nova/nova.conf 文件， 下面是我的nova.conf 文件的配置。 为了简单，大家直接copy下面内容，运行就可以，注意修改ip。 如果你是在虚拟机里安装，你需要把libvirt_type=kvm 改成 ibvirt_type=qemu [DEFAULT] dhcpbridge_flagfile=/etc/nova/nova.conf dhcpbridge=/usr/bin/nova-dhcpbridge logdir=/var/log/nova state_path=/var/lib/nova lock_path=/var/lock/nova force_dhcp_release=True iscsi_helper=tgtadm libvirt_use_virtio_for_bridges=True connection_type=libvirt root_helper=sudo nova-rootwrap /etc/nova/rootwrap.conf verbose=True ec2_private_dns_show_ip=True api_paste_config=/etc/nova/api-paste.ini volumes_path=/var/lib/nova/volumes enabled_apis=ec2,osapi_compute,metadata rpc_backend = rabbit rabbit_host = 10.210.0.94 my_ip = 10.210.0.94 vncserver_listen = 10.210.0.94 vncserver_proxyclient_address = 10.210.0.94 auth_strategy = keystone [keystone_authtoken] auth_uri = http://10.210.0.94:5000 auth_host = 10.210.0.94 auth_port = 35357 auth_protocol = http admin_tenant_name = service admin_user = nova admin_password = password [database] connection = mysql://nova:password@10.210.0.94/nova 设置目录权限 chown -R nova:nova /etc/nova 重启所有服务 service rabbitmq-server restart service libvirt-bin restart service nova-scheduler restart service nova-network restart service nova-cert restart service nova-compute restart service nova-api restart service nova-objectstore restart service nova-volume restart 也可以使用下面脚本进行重启 #!/bin/bash for a in rabbitmq-server libvirt-bin nova-network nova-cert nova-compute \\ nova-api nova-objectstore nova-scheduler nova-volume \\ novnc nova-consoleauth; do service &quot;$a&quot; stop; done for a in rabbitmq-server libvirt-bin nova-network nova-cert nova-compute \\ nova-api nova-objectstore nova-scheduler nova-volume \\ novnc nova-consoleauth; do service &quot;$a&quot; start; done 运行脚本 bash restart.sh Stopping rabbitmq-server: rabbitmq-server. libvirt-bin stop/waiting nova-network stop/waiting nova-cert stop/waiting nova-compute stop/waiting nova-api stop/waiting nova-objectstore stop/waiting nova-scheduler stop/waiting nova-volume stop/waiting * Stopping OpenStack NoVNC proxy nova-novncproxy [ OK ] nova-consoleauth stop/waiting Starting rabbitmq-server: SUCCESS rabbitmq-server. libvirt-bin start/running, process 9683 nova-network start/running, process 9703 nova-cert start/running, process 9713 nova-compute start/running, process 9724 nova-api start/running, process 9734 nova-objectstore start/running, process 9744 nova-scheduler start/running, process 9759 nova-volume start/running, process 9775 * Starting OpenStack NoVNC proxy nova-novncproxy [ OK ] nova-consoleauth start/running, process 9839 同步数据库 nova-manage db sync 会有一堆的输出，不过应该是没问题的。nova数据库里已经有相应的表，就表示正确。 # nova-manage db sync 2012-07-19 18:43:34 WARNING nova.utils [-] /usr/lib/python2.7/dist-packages/sqlalchemy/pool.py:639: SADeprecationWarning: The &apos;listeners&apos; argument to Pool (and create_engine()) is deprecated. Use event.listen(). Pool.__init__(self, creator, **kw) 2012-07-19 18:43:34 WARNING nova.utils [-] /usr/lib/python2.7/dist-packages/sqlalchemy/pool.py:145: SADeprecationWarning: Pool.add_listener is deprecated. Use event.listen() self.add_listener(l) 2012-07-19 18:43:34 AUDIT nova.db.sqlalchemy.fix_dns_domains [-] Applying database fix for Essex dns_domains table. 创建fix IPFIX IP，就是分配给虚拟机的实际IP地址。这些数据都会写入数据库。$FIXED_RANGE 在novarc里设置。 nova-manage network create private --fixed_range_v4=$FIXED_RANGE \\ --num_networks=1 --bridge=br100 --bridge_interface=eth1 \\ --network_size=256 --multi_host=T 创建floating IP所谓Floating IP，是亚马逊EC2的定义。简单说，就是公网的IP。他其实是通过类似防火墙类似，做一个映射。实际上是通过iptables来实现映射. nova-manage floating create --ip_range=$FLOATING_RANGE 重启nova服务 bash restart.sh libvirt-bin stop/waiting nova-network stop/waiting nova-cert stop/waiting nova-compute stop/waiting nova-api stop/waiting nova-objectstore stop/waiting nova-scheduler stop/waiting nova-volume stop/waiting * Stopping OpenStack NoVNC proxy nova-novncproxy [ OK ] nova-consoleauth stop/waiting libvirt-bin start/running, process 23232 nova-network start/running, process 23252 nova-cert start/running, process 23262 nova-compute start/running, process 23273 nova-api start/running, process 23285 nova-objectstore start/running, process 23303 nova-scheduler start/running, process 23321 nova-volume start/running, process 23336 * Starting OpenStack NoVNC proxy nova-novncproxy [ OK ] nova-consoleauth start/running, process 23386 测试可以尝试用下面命令去检查nova的状况 nova-manage service list 若state均为:-） \\说明安装成功 命令行创建虚拟机的过程nova keypair-add oskey &gt; oskey.priv chmod 600 oskey.priv nova flavor-list nova image-list nova boot --flavor 2 --key_name oskey --image ea3ffba1-065e-483f-bfe2-c84184ee76be test1 nova secgroup-add-rule default tcp 22 22 0.0.0.0/0 nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0 这个时候，你在服务器上可以直接ssh到虚拟机上，ubuntu的虚拟机，用户是ubuntu。虚拟机的Ip # nova list +--------------------------------------+-------+--------+------------------+ | ID | Name | Status | Networks | +--------------------------------------+-------+--------+------------------+ | 61e93d62-c926-46fa-8e0c-48073b7e58b0 | test1 | ACTIVE | private=10.0.0.2 | | 6976e539-32d9-48a6-9fb5-28a3cdb55f71 | test2 | ACTIVE | private=10.0.0.4 | +--------------------------------------+-------+--------+------------------+ 在服务器上直接ssh到虚拟机，如果你在远程，就需要分配floating IP。 ssh -i oskey.priv ubuntu@10.0.0.4 登陆虚拟机后，你可以查看一下路由 $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default 10.0.0.3 0.0.0.0 UG 100 0 0 eth0 10.0.0.0 * 255.255.255.0 U 0 0 0 eth0 显示网关是10.0.0.3，这个时候，你看一下 root@node:~# ifconfig br100 Link encap:Ethernet HWaddr 00:e0:81:d8:4a:23 inet addr:10.0.0.3 Bcast:10.0.0.255 Mask:255.255.255.0 inet6 addr: fe80::ccfc:5aff:fef5:4345/64 Scope:Link 需要注意的是：br100的IP，需要你创建第一个虚拟机，他才会获得IP。 Dashobard安装apt-get install -y apache2 libapache2-mod-wsgi openstack-dashboard 重启nova api restart nova-api 这个时候，就可以访问dashboard。 测试登陆dashobard，下面地址是我的horizon界面，可以直接访问，注：需要北航内网。 http://10.210.0.94/horizon \\user:admin \\pass:password","categories":[],"tags":[{"name":"OpenStack","slug":"OpenStack","permalink":"http://yoursite.com/tags/OpenStack/"}]},{"title":"你好，Hexo","slug":"你好，Hexo","date":"2018-05-13T14:23:13.000Z","updated":"2018-05-13T14:23:13.092Z","comments":true,"path":"2018/05/13/你好，Hexo/","link":"","permalink":"http://yoursite.com/2018/05/13/你好，Hexo/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2018-05-13T13:58:28.016Z","updated":"2018-05-13T13:58:28.016Z","comments":true,"path":"2018/05/13/hello-world/","link":"","permalink":"http://yoursite.com/2018/05/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}